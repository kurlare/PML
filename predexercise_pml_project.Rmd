---
title: "Predicting Exercise Performance With the Quantified Self"
author: "Rafi Kurlansik"
date: "October 20, 2015"
output: html_document
---

```{r, echo=FALSE, warning = FALSE, message = FALSE}
require(ggplot2)
require(dplyr)
require(randomForest)
require(caret)
require(gridExtra)
```


```{r read in, cache=TRUE, echo = FALSE}
training <- read.csv(
    url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),
    row.names = NULL)

testing_final <- read.csv(
    url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),
    row.names = NULL)

```

####Summary

The advent of wearable technology has allowed for the collection of human movement data. In an attempt to quantify how well an exercise is performed, a team of researchers used on-body and wearable sensors to record the motion of a specific exercise. These readings were captured using the accelerometer, gyroscope, and magnetometer of sensors places on the arm, forearm, belt, and dumbbell. With the data they collected, we will work to select a machine learning algorithm that will correctly classify new exercise data with the highest accuracy. The original paper can be found [here](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf), and the data [here](http://groupware.les.inf.puc-rio.br/static/WLE/WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv).

####Exploratory Data Analysis

The weight exercise data set is fairly complex, with over 19,000 observations and 160 variables. This structure is too large to display cleanly, so we will merely show a few sample columns. All of the variables are essentially some variation of the ones we are about to see.  Note: `testing_final` is a set of 20 observations of the 160 variables where we do *not* know the class.  This is the second part of the project assignment, and will be evaluated at the very end.  

```{r, collapse = TRUE}
dim(training)
dim(testing_final)
col_samp <- training[1:20, grep("classe|total|user|time|belt_x", colnames(training))]
head(col_samp[, c(1,4,6,7,13)])
```
We see the user name, a time stamp for the activity, a sensor summary statistic, a sensor measurement, and the classification `classe`. The classification is a factor variable with 5 levels, each corresponding to a different activity:

* Class A - Exactly according to specification (i.e., doing the exercise correctly).
* Class B - Throwing the Elbow to the front.
* Class C - Lifting the Dumbbell only halfway.
* Class D - Lowering the Dumbbell only halfway.
* Class E - Throwing the Hips to the front.

`classe` is the outcome we will ultimately want our model to predict, but for now there is more exploring to do.

####Missing Values

When we check for NA’s, we see there are quite a lot. Upon further inspection, we see that the testing_final set is missing all rows in multiple columns. Here is one column, and then the sum of columns with all NA’s:

```{r, collapse = TRUE}
sum(colSums(is.na(training)))
sum(colSums(is.na(testing_final)))
testing_final$kurtosis_roll_belt
sum(colSums(is.na(testing_final)) == nrow(testing_final))
```

In fact, 100 of its 160 columns have no data in them! This makes prediction with those variables impossible, so we will drop them now.  Later, we will use this information to inform feature selection.

```{r}
## Select only those columns whose NA's are not equal to number of rows in df
testing_final <- testing_final[, colSums(is.na(testing_final)) != nrow(testing_final)]
```

___________________________________________________________________________________

####Visualizing Total Acceleration by Class

With so many variables and so many possible relationships, visually identifying some patterns is a challenge. One approach is to plot the densities of the `total_accel_ group` of variables and get and idea of how acceleration differs at each sensor, for each class.

```{r, echo = FALSE}

belt_accel_plot <- ggplot(training, aes(x = training$total_accel_belt,
                          colour = classe, fill = classe)) + 
                          geom_density(alpha = 0.3) +
                          labs(title = "Belt", x = "Total Belt Accel",
                            y = "Density")

arm_accel_plot <- ggplot(training, aes(x = training$total_accel_arm, 
                         colour = classe, fill = classe)) + 
                         geom_density(alpha = 0.3) +
                         labs(title = "Arm", x = "Total Arm Accel",
                            y = "Density")

forearm_accel_plot <- ggplot(training, aes(x = training$total_accel_forearm,
                         colour = classe, fill = classe)) +
                         geom_density(alpha = 0.3) +
                         labs(title = "Forearm", x = "Total Forearm Accel",
                             y = "Density")

dumbbell_accel_plot <- ggplot(training, aes(x = training$total_accel_dumbbell,
                         colour = classe, fill = classe)) +
                         geom_density(alpha = 0.3) +
                         labs(title = "Dumbbell", x = "Total Dumbbell Accel",
                             y = "Density")

grid.arrange(belt_accel_plot, 
             arm_accel_plot, 
             forearm_accel_plot, 
             dumbbell_accel_plot, 
             nrow = 2)

```

There are several interpretations we can make with this figure:

- Belt acceleration is highest for the ‘E’ class, whose description is ‘throwing hips to the front’. It also has an otherwise clear bi-modal distribution (you either are really moving your hips or you aren’t moving them much at all), with the A class clearly distributed closer to 0.

-  Arm acceleration is mostly consistent across different movements, with the exception of lowering the dumbbell only halfway (D) and throwing the hips forward (E).

-  Forearm acceleration appears to have relatively similiar distributions across all classes, with less variability in the correct motion.

-  Dumbbell acceleration is lower for motions that restrict dumbbell motion (C and D), and higher for full, correct motion (A).

The data reflects the classification variables nicely, and this will give us some confidence that sensor variables besides `total_accel` will accurately reflect the different movements.

________________________________________________________________________________________________

####Feature Selection

As previously mentioned, 100 of the testing predictors have no values in their rows. To build our training set we can select those columns that match the remaining columns in the test set. We will also remove variables for `X`, which appears to be redundant row names, `user_name`, and the time series variables.

An argument can be made that time series data should be included as each movement is a process with a beginning and end. However, as our density plot above shows, the raw sensor measurements do appear to capture the movements for each class. In the interest of simplicity, we will roll with that.

```{r, collapse=TRUE}
classe <- training$classe ## Remove classification before matching
matches_index <- match(colnames(testing_final), colnames(training))
training <- training[, matches_index[-60]]  ## Subset for matches, without NA column `problem_id`
training <- training[-c(1,2,3,4,5,6,7)]     ## Remove username, X, and time series
training$classe <- classe   ## Put back classifications
```

As a final test, we can call the `nearZeroVar` function to see if any of our remaining predictors have little variability, and thus little information to contribute to the model.

```{r, collapse=TRUE}
nsv <- nearZeroVar(training, saveMetrics = T)
unique(nsv$nzv)
```

No near zero variance predictors. Good!

_____________________________________________________________________________________________

####Model Selection

Moving forward, we have narrowed down our predictors from 159 to 52. We will use randomForest as our model. Random forest is a good choice for this data set because we are mostly interested in prediction, not interpretability. Cross validation is achieved within the random forest algorithm itself.

Each tree is built using a random sampling of our original predictors in the training set. Some of the rows for predictors of the tree are held out, and this group is called the out-of-bag sample. When the tree is fully grown and there are no more splits to be made, the OOB samples are run through the tree. How accurately the tree predicts these OOB samples yields an error rate. The OOB error rate reported in the final model is the rate for how all trees, in aggregate, predicted the OOB samples. _This is akin to separating data into train and test sets to estimate the out-of-sample error rate._

In spite of this fact, as an exercise we will split our data into training and test sets, and evaluate our model accordingly.
___________________________________________________________________________________________

To construct the model:

```{r model, cache = TRUE, collapse = TRUE}
set.seed(206)
inTrain <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)  ## Split data
training <- training[inTrain, ]  ## Create training set
testing <- training[-inTrain, ]  ## Create test set
fit.rf <- randomForest(classe ~ ., data = training, importance = TRUE)  ## Fit model to training set
fit.rf
```

Accuracy looks promising, with an OOB estimate of error rate of 0.28%!  Before moving to prediction, lets look at the importance of each variable.  The plot below will show us that the variables which decrease model accuracy the most when left out.  If we wanted, we could tweak our final model by removing the less important variables, or perhaps select some upper quantile of important variables.

```{r}
varImpPlot(fit.rf)
```

#### Prediction

We are now in a position to apply our `fit.rf` model to the testing set.  

```{r}
pred <- predict(fit.rf,testing)
testing$predRight <- pred == testing$classe
table(pred, testing$classe)
```

To be honest, I am _very_ surprised at the accuracy of this model.  We would expect the OOB error to be greater on the testing set, and here there are no errors.  

In an attempt to figure out what might be wrong I rebuilt the model in an iterative fashion, first introducing the most important feature, then the top 5, top 10.  In the first iteration, the error rate was 44.6%, the next, 4.7%, and finally 1.67%.  So it stands to reason that the OOB estimate of the error rate with all 52 features would be close to zero.  This is also supported by the fact that none of them possess near zero variability.

Before we conclude, lets check the densities on the test set to see if they reflect what we saw in the training set. 

```{r, echo = FALSE}
belt_accel_plot <- ggplot(testing, aes(x = testing$total_accel_belt,
                                        colour = classe, fill = classe)) + 
    geom_density(alpha = 0.3) +
    labs(title = "Belt (Test)", x = "Total Belt Accel",
         y = "Density")

arm_accel_plot <- ggplot(testing, aes(x = testing$total_accel_arm, 
                                       colour = classe, fill = classe)) + 
    geom_density(alpha = 0.3) +
    labs(title = "Arm (Test)", x = "Total Arm Accel",
         y = "Density")

forearm_accel_plot <- ggplot(testing, aes(x = testing$total_accel_forearm,
                                           colour = classe, fill = classe)) +
    geom_density(alpha = 0.3) +
    labs(title = "Forearm (Test)", x = "Total Forearm Accel",
         y = "Density")

dumbbell_accel_plot <- ggplot(testing, aes(x = testing$total_accel_dumbbell,
                                            colour = classe, fill = classe)) +
    geom_density(alpha = 0.3) +
    labs(title = "Dumbbell (Test)", x = "Total Dumbbell Accel",
         y = "Density")

grid.arrange(belt_accel_plot, 
             arm_accel_plot, 
             forearm_accel_plot, 
             dumbbell_accel_plot, 
             nrow = 2)

```

The classification appears to be accurate when we visualize the data again.  

#### Conclusion

In conclusion, the `fit.rf` model is adequately tuned to give accurate predictions for how well a human lifts a dumbbell, provided they wear the sensors outlined in the original research paper.

In fact, when this model was applied to the `testing_final` data for the second part of the project, it achieved 100% accuracy there as well.  

_________________________________________________________________